{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from roboflow import Roboflow\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\DAFFA PINJEM BUAT BACKUP\\\\TA GASS\\\\FIX\\\\Yolo'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = (os.getcwd())\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = './Multi-Object/'\n",
    "target_dir = './Multi-Object-Result/'\n",
    "predict_dir = './Predict/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "# Initialize API for coffee bean detection\n",
    "rf = Roboflow(api_key=\"IluK8shPQ2JbRq6f5FVM\")\n",
    "\n",
    "# Reference : https://universe.roboflow.com/coffe-dataset/coffee-beans-znwfe/model/2\n",
    "project = rf.workspace().project(\"coffee-beans-znwfe\")\n",
    "model = project.version(2).model\n",
    "confident = 40\n",
    "overlap = 30\n",
    "\n",
    "def coffee_detection(filename):\n",
    "  # infer on a local image\n",
    "  result = model.predict(f\"{source_dir}{filename}\", confidence=confident, overlap=overlap).json()\n",
    "\n",
    "  # visualize your prediction\n",
    "  # model.predict(f\"{source_dir}{filename}\", confidence=confident, overlap=overlap).save(f\"{target_dir}{filename}\")\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_model(num_classes):\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, num_classes)  # Ganti num_classes dengan jumlah kelas dalam dataset Anda\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, model):\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n"
     ]
    }
   ],
   "source": [
    "resnet50 = get_pretrained_model(2)\n",
    "\n",
    "defect_label_list = [\"Defect\", \"Normal\"]\n",
    "defect_path = 'resnet50-coffee-defect.pth'\n",
    "defect_model = load_model(defect_path, resnet50)\n",
    "\n",
    "resnet50 = get_pretrained_model(4)\n",
    "\n",
    "roast_label_list = ['Dark', 'Green', 'Light', 'Medium']\n",
    "roastlevel_path = 'resnet50-coffee-roast-level.pth'\n",
    "roastlevel_model = load_model(roastlevel_path, resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Defect 100%-0.jpg', 'Defect 100%.JPG', (1848, 2043, 639, 448)), ('Defect 100%-1.jpg', 'Defect 100%.JPG', (1171, 1686, 548, 448)), ('Defect 100%-2.jpg', 'Defect 100%.JPG', (1420, 2346, 548, 440)), ('Defect 100%-3.jpg', 'Defect 100%.JPG', (847, 1154, 581, 415)), ('Defect 100%-4.jpg', 'Defect 100%.JPG', (847, 2035, 548, 398)), ('Defect 100%-5.jpg', 'Defect 100%.JPG', (2583, 1948, 548, 390)), ('Defect 100%-6.jpg', 'Defect 100%.JPG', (1998, 1428, 490, 398)), ('Defect 100%-7.jpg', 'Defect 100%.JPG', (2720, 1237, 456, 498)), ('Defect 100%-8.jpg', 'Defect 100%.JPG', (2081, 847, 606, 398)), ('Defect 50% Normal 50%-0.jpg', 'Defect 50% Normal 50%.JPG', (1827, 992, 498, 540)), ('Defect 50% Normal 50%-1.jpg', 'Defect 50% Normal 50%.JPG', (2587, 731, 606, 465)), ('Defect 50% Normal 50%-2.jpg', 'Defect 50% Normal 50%.JPG', (2592, 2234, 548, 448)), ('Defect 50% Normal 50%-3.jpg', 'Defect 50% Normal 50%.JPG', (1183, 801, 606, 506)), ('Defect 50% Normal 50%-4.jpg', 'Defect 50% Normal 50%.JPG', (2700, 1636, 531, 498)), ('Defect 50% Normal 50%-5.jpg', 'Defect 50% Normal 50%.JPG', (1512, 1815, 631, 473)), ('Defect 50% Normal 50%-6.jpg', 'Defect 50% Normal 50%.JPG', (2114, 1661, 523, 415)), ('Defect 50% Normal 50%-7.jpg', 'Defect 50% Normal 50%.JPG', (888, 2446, 465, 540)), ('Defect 50% Normal 50%-8.jpg', 'Defect 50% Normal 50%.JPG', (697, 1653, 548, 498)), ('Medium 100%-0.jpg', 'Medium 100%.JPG', (1005, 1225, 648, 540)), ('Medium 100%-1.jpg', 'Medium 100%.JPG', (2351, 2064, 564, 506)), ('Medium 100%-2.jpg', 'Medium 100%.JPG', (814, 2081, 631, 490)), ('Medium 100%-3.jpg', 'Medium 100%.JPG', (2243, 793, 648, 523)), ('Medium 100%-4.jpg', 'Medium 100%.JPG', (1869, 1445, 648, 465)), ('Medium 100%-5.jpg', 'Medium 100%.JPG', (2712, 1399, 556, 456)), ('Medium 100%-6.jpg', 'Medium 100%.JPG', (1968, 2579, 531, 556)), ('Medium 100%-7.jpg', 'Medium 100%.JPG', (1395, 1927, 598, 515)), ('Medium 100%-8.jpg', 'Medium 100%.JPG', (1262, 2488, 548, 506)), ('Medium 100%-9.jpg', 'Medium 100%.JPG', (1528, 664, 548, 481)), ('Medium 30% Defect 30% Normal 40%-0.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (2010, 2409, 714, 481)), ('Medium 30% Defect 30% Normal 40%-1.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (540, 1325, 564, 473)), ('Medium 30% Defect 30% Normal 40%-2.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (2193, 947, 631, 415)), ('Medium 30% Defect 30% Normal 40%-3.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (1237, 1769, 515, 598)), ('Medium 30% Defect 30% Normal 40%-4.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (2666, 1919, 564, 332)), ('Medium 30% Defect 30% Normal 40%-5.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (510, 2193, 523, 498)), ('Medium 30% Defect 30% Normal 40%-6.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (2745, 1320, 540, 448)), ('Medium 30% Defect 30% Normal 40%-7.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (1183, 2612, 473, 589)), ('Medium 30% Defect 30% Normal 40%-8.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (1948, 1574, 473, 556)), ('Medium 30% Defect 30% Normal 40%-9.jpg', 'Medium 30% Defect 30% Normal 40%.JPG', (1296, 747, 498, 531)), ('Medium 30% Defect 40% Normal 30%-0.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (2716, 2317, 598, 448)), ('Medium 30% Defect 40% Normal 30%-1.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (942, 1084, 573, 506)), ('Medium 30% Defect 40% Normal 30%-2.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (789, 2454, 531, 473)), ('Medium 30% Defect 40% Normal 30%-3.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (2446, 1615, 573, 506)), ('Medium 30% Defect 40% Normal 30%-4.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (1910, 1420, 681, 415)), ('Medium 30% Defect 40% Normal 30%-5.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (2862, 1192, 473, 440)), ('Medium 30% Defect 40% Normal 30%-6.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (2085, 780, 631, 498)), ('Medium 30% Defect 40% Normal 30%-7.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (1707, 2010, 556, 531)), ('Medium 30% Defect 40% Normal 30%-8.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (1898, 2583, 473, 498)), ('Medium 30% Defect 40% Normal 30%-9.jpg', 'Medium 30% Defect 40% Normal 30%.JPG', (702, 1806, 573, 490)), ('Medium 40% Defect 30% Normal 30%-0.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (2575, 2484, 631, 432)), ('Medium 40% Defect 30% Normal 30%-1.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (2446, 1084, 589, 506)), ('Medium 40% Defect 30% Normal 30%-2.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (1769, 2558, 515, 564)), ('Medium 40% Defect 30% Normal 30%-3.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (2214, 1939, 573, 490)), ('Medium 40% Defect 30% Normal 30%-4.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (1208, 2226, 573, 432)), ('Medium 40% Defect 30% Normal 30%-5.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (1836, 901, 515, 506)), ('Medium 40% Defect 30% Normal 30%-6.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (2820, 1703, 573, 315)), ('Medium 40% Defect 30% Normal 30%-7.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (876, 1242, 556, 440)), ('Medium 40% Defect 30% Normal 30%-8.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (1449, 1482, 573, 506)), ('Medium 40% Defect 30% Normal 30%-9.jpg', 'Medium 40% Defect 30% Normal 30%.JPG', (710, 1977, 506, 332)), ('Medium 50% Defect 50%-0.jpg', 'Medium 50% Defect 50%.JPG', (797, 913, 598, 465)), ('Medium 50% Defect 50%-1.jpg', 'Medium 50% Defect 50%.JPG', (1669, 793, 531, 523)), ('Medium 50% Defect 50%-2.jpg', 'Medium 50% Defect 50%.JPG', (2708, 814, 564, 531)), ('Medium 50% Defect 50%-3.jpg', 'Medium 50% Defect 50%.JPG', (1678, 1811, 564, 548)), ('Medium 50% Defect 50%-4.jpg', 'Medium 50% Defect 50%.JPG', (2737, 1723, 523, 589)), ('Medium 50% Defect 50%-5.jpg', 'Medium 50% Defect 50%.JPG', (506, 2018, 531, 465)), ('Medium 50% Defect 50%-6.jpg', 'Medium 50% Defect 50%.JPG', (2097, 2284, 490, 531)), ('Medium 50% Defect 50%-7.jpg', 'Medium 50% Defect 50%.JPG', (1200, 2446, 672, 440)), ('Medium 50% Defect 50%-8.jpg', 'Medium 50% Defect 50%.JPG', (1001, 1603, 540, 465)), ('Medium 50% Normal 50%-0.jpg', 'Medium 50% Normal 50%.JPG', (2646, 685, 556, 556)), ('Medium 50% Normal 50%-1.jpg', 'Medium 50% Normal 50%.JPG', (1329, 942, 531, 556)), ('Medium 50% Normal 50%-2.jpg', 'Medium 50% Normal 50%.JPG', (1644, 1628, 564, 598)), ('Medium 50% Normal 50%-3.jpg', 'Medium 50% Normal 50%.JPG', (1217, 2334, 639, 465)), ('Medium 50% Normal 50%-4.jpg', 'Medium 50% Normal 50%.JPG', (2222, 1109, 556, 473)), ('Medium 50% Normal 50%-5.jpg', 'Medium 50% Normal 50%.JPG', (577, 1104, 556, 564)), ('Medium 50% Normal 50%-6.jpg', 'Medium 50% Normal 50%.JPG', (2683, 1748, 515, 556)), ('Medium 50% Normal 50%-7.jpg', 'Medium 50% Normal 50%.JPG', (702, 1927, 606, 415)), ('Medium 50% Normal 50%-8.jpg', 'Medium 50% Normal 50%.JPG', (2421, 2608, 506, 531)), ('Medium 50% Normal 50%-9.jpg', 'Medium 50% Normal 50%.JPG', (2101, 2189, 498, 540)), ('Normal 100%-0.jpg', 'Normal 100%.JPG', (913, 847, 481, 564)), ('Normal 100%-1.jpg', 'Normal 100%.JPG', (2754, 1217, 589, 473)), ('Normal 100%-2.jpg', 'Normal 100%.JPG', (2712, 2197, 490, 540)), ('Normal 100%-3.jpg', 'Normal 100%.JPG', (2110, 1590, 531, 573)), ('Normal 100%-4.jpg', 'Normal 100%.JPG', (818, 1715, 589, 456)), ('Normal 100%-5.jpg', 'Normal 100%.JPG', (1981, 2596, 523, 506)), ('Normal 100%-6.jpg', 'Normal 100%.JPG', (1603, 2122, 581, 473)), ('Normal 100%-7.jpg', 'Normal 100%.JPG', (876, 2400, 473, 648)), ('Normal 100%-8.jpg', 'Normal 100%.JPG', (1939, 656, 473, 481)), ('Prediction Test-0.jpg', 'Prediction Test.JPG', (627, 1752, 589, 432)), ('Prediction Test-1.jpg', 'Prediction Test.JPG', (1989, 2434, 506, 432)), ('Prediction Test-2.jpg', 'Prediction Test.JPG', (2463, 1391, 506, 440)), ('Prediction Test-3.jpg', 'Prediction Test.JPG', (1013, 2467, 598, 382)), ('Prediction Test-4.jpg', 'Prediction Test.JPG', (1536, 1669, 548, 432)), ('Prediction Test-5.jpg', 'Prediction Test.JPG', (1603, 959, 564, 506)), ('Prediction Test-6.jpg', 'Prediction Test.JPG', (2313, 859, 573, 423)), ('Prediction Test-7.jpg', 'Prediction Test.JPG', (897, 1225, 498, 490)), ('Testing-0.jpg', 'Testing.jpg', (1167, 1208, 656, 523)), ('Testing-1.jpg', 'Testing.jpg', (1960, 1690, 598, 573)), ('Testing-2.jpg', 'Testing.jpg', (1271, 340, 631, 481)), ('Testing-3.jpg', 'Testing.jpg', (2110, 502, 581, 573)), ('Testing-4.jpg', 'Testing.jpg', (681, 2301, 548, 548)), ('Testing-5.jpg', 'Testing.jpg', (1503, 2670, 581, 540)), ('Testing-6.jpg', 'Testing.jpg', (2853, 1516, 490, 506)), ('Testing-7.jpg', 'Testing.jpg', (2866, 598, 614, 382)), ('Testing-8.jpg', 'Testing.jpg', (2488, 2459, 523, 498)), ('Testing-9.jpg', 'Testing.jpg', (448, 1624, 498, 573)), ('Testing-10.jpg', 'Testing.jpg', (369, 913, 440, 481))]\n"
     ]
    }
   ],
   "source": [
    "file_images = os.listdir(source_dir)\n",
    "img_list = []\n",
    "\n",
    "for file_image in file_images:\n",
    "    result = coffee_detection(file_image)\n",
    "\n",
    "    # Open the image file\n",
    "    for index, predict in enumerate(result[\"predictions\"]):\n",
    "        img = Image.open(os.path.join(source_dir, file_image))\n",
    "\n",
    "        # Set the crop region\n",
    "        x, y, width, height = predict[\"x\"], predict[\"y\"], predict[\"width\"], predict[\"height\"]\n",
    "        max_width = max(width, height)\n",
    "        crop_region = (x-(int(max_width/2)), y-(int(max_width/2)), x+(int(max_width/2)), y+(int(max_width/2)))\n",
    "\n",
    "        # Crop the image\n",
    "        cropped_img = img.crop(crop_region)\n",
    "\n",
    "        # Save cropped image\n",
    "        cropped_filename = f\"{file_image.split('.')[0]}-{index}.jpg\"\n",
    "        cropped_img.save(f\"./Predict/{cropped_filename}\")\n",
    "        img_tuple = (cropped_filename, file_image, (x, y, width, height))\n",
    "        img_list.append(img_tuple)\n",
    "\n",
    "        # Display the original and cropped images\n",
    "        # fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        # ax1.imshow(img)\n",
    "        # ax1.set_title('Original Image')\n",
    "        # ax2.imshow(cropped_img)\n",
    "        # ax2.set_title('Cropped Image')\n",
    "        # plt.show()\n",
    "\n",
    "print(img_list)\n",
    "\n",
    "# with open('result.json', 'w') as f:\n",
    "#     json.dump(result_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the images and make predictions\n",
    "for img_predict in img_list:\n",
    "    # load the image and apply the transformations\n",
    "    image = Image.open(os.path.join(predict_dir, img_predict[0]))\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # make a prediction\n",
    "    with torch.no_grad():\n",
    "        defect_output = defect_model(image)\n",
    "        # get the predicted class label\n",
    "        predicted_defect = np.argmax(defect_output.numpy())\n",
    "        defect_label = defect_label_list[predicted_defect.item()]\n",
    "\n",
    "        roast_output = roastlevel_model(image)\n",
    "        # get the predicted class label\n",
    "        predicted_roast = np.argmax(roast_output.numpy())\n",
    "        roast_label = roast_label_list[predicted_roast.item()]\n",
    "\n",
    "        if not os.path.exists(os.path.join(target_dir, img_predict[1])):\n",
    "            img_result = cv2.imread(os.path.join(source_dir, img_predict[1]))\n",
    "        else:\n",
    "            img_result = cv2.imread(os.path.join(target_dir, img_predict[1]))\n",
    "\n",
    "        x, y, w, h = img_predict[2]\n",
    "\n",
    "        # Draw bounding box on the image\n",
    "        cv2.rectangle(img_result, (x - int(w/2), y - int(h/2)), (x + int(w/2), y + int(h/2)), (0, 0, 255), 2)\n",
    "\n",
    "        # Add label to the bounding box\n",
    "        cv2.putText(img_result, f\"{roast_label} - {defect_label}\", (x - int(w/2), y - int(h/2)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Save the img_result with bounding box and label\n",
    "        cv2.imwrite(os.path.join(target_dir, img_predict[1]), img_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
